{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:54:13.107112600Z",
     "start_time": "2024-02-27T11:53:58.717056300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from lightning import LightningModule, Trainer, LightningDataModule\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from rdkit.Chem import MolFromSmiles, DataStructs\n",
    "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
    "from pandas import read_csv\n",
    "from itertools import chain\n",
    "from pickle import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class DrugDrugCliffNN(LightningModule):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # The branch for processing each compound\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # The classifier part that operates on the concatenated output of compound branches\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, drug1, drug2):\n",
    "        # Process each compound through the same branch\n",
    "        drug1_out = self.encoder(drug1)\n",
    "        drug2_out = self.encoder(drug2)\n",
    "\n",
    "        # Concatenate the outputs\n",
    "        combined_out = torch.cat((drug1_out, drug2_out), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        return self.classifier(combined_out).flatten()\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        drug1, drug2, clf = batch\n",
    "\n",
    "        preds = self(drug1, drug2)\n",
    "        ls = F.binary_cross_entropy_with_logits(preds, clf)\n",
    "        self.log('Training/BCELoss', ls)\n",
    "        return ls\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        drug1, drug2, clf = batch\n",
    "\n",
    "        preds = self(drug1, drug2)\n",
    "        ls = F.binary_cross_entropy_with_logits(preds, clf)\n",
    "        self.log('Validation/BCELoss', ls)\n",
    "\n",
    "    def test_step(self, batch, *_):\n",
    "        drug1, drug2, clf = batch\n",
    "\n",
    "        preds = self(drug1, drug2)\n",
    "        ls = F.binary_cross_entropy_with_logits(preds, clf)\n",
    "        self.log('Test/BCELoss', ls)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:58:10.529969100Z",
     "start_time": "2024-02-27T11:58:10.504843300Z"
    }
   },
   "id": "d05724dba858ebbc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class DrugDrugData(LightningDataModule):\n",
    "    def __init__(self, csv, radius=2, n_bits=1024, cache='cache.data', batch_size=10):\n",
    "        super().__init__()\n",
    "        self.prepare_data_per_node = False\n",
    "        self.csv = csv\n",
    "        self.radius = radius\n",
    "        self.n_bits = n_bits\n",
    "        self.cache = cache\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # expected next columns: smiles1, smiles2, cliff, split\n",
    "        data = read_csv(self.csv)\n",
    "\n",
    "        cache = {}\n",
    "        for s in chain(data.smiles1, data.smiles2):\n",
    "            if s in cache:\n",
    "                continue\n",
    "            mol = MolFromSmiles(s)\n",
    "            fp = GetMorganFingerprintAsBitVect(mol, self.radius, nBits=self.n_bits)\n",
    "            arr = np.zeros((0,), dtype=np.int8)\n",
    "            DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "            cache[s] = torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "        drugs1, drugs2 = [], []\n",
    "        for row in data.itertuples():\n",
    "            drugs1.append(cache[row.smiles1])\n",
    "            drugs2.append(cache[row.smiles2])\n",
    "        drugs1 = torch.stack(drugs1)\n",
    "        drugs2 = torch.stack(drugs2)\n",
    "        cliff = torch.tensor(data.cliff, dtype=torch.float32)\n",
    "        split = torch.tensor(data.split, dtype=torch.int8)\n",
    "        with open(self.cache, 'wb') as f:\n",
    "            dump((drugs1, drugs2, cliff, split), f)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        with open(self.cache, 'rb') as f:\n",
    "            drugs1, drugs2, cliff, split = load(f)\n",
    "\n",
    "        mask = split == 0\n",
    "        self._train = TensorDataset(drugs1[mask], drugs2[mask], cliff[mask])\n",
    "        mask = split == 1\n",
    "        self._validation = TensorDataset(drugs1[mask], drugs2[mask], cliff[mask])\n",
    "        mask = split == 2\n",
    "        self._test = TensorDataset(drugs1[mask], drugs2[mask], cliff[mask])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self._train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self._validation, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self._test, batch_size=self.batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:54:22.387523200Z",
     "start_time": "2024-02-27T11:54:22.335216200Z"
    }
   },
   "id": "7e5b690876dbb769"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data = DrugDrugData('../analysis/kiba_cliff_pairs_ta_1_ts_0.9_cb.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:54:35.997980800Z",
     "start_time": "2024-02-27T11:54:35.940447400Z"
    }
   },
   "id": "19c1694ee7329f07"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "model = DrugDrugCliffNN()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:58:23.181677500Z",
     "start_time": "2024-02-27T11:58:23.129991100Z"
    }
   },
   "id": "a4a3a974d440255b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: W&B API key is configured. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\rena_\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key='fd8f6e44f8d81be3a652dbd8f4a47a7edf59e44c')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:56:26.980584800Z",
     "start_time": "2024-02-27T11:56:19.648025800Z"
    }
   },
   "id": "943ee7c594c3835"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mreginaib\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777774052488, max=1.0â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7f0a0a964a54436a3e72c508e27dd67"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.3"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>.\\wandb\\run-20240227_125706-ztgff617</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/reginaib/kiba_cb/runs/ztgff617' target=\"_blank\">sandy-dew-1</a></strong> to <a href='https://wandb.ai/reginaib/kiba_cb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/reginaib/kiba_cb' target=\"_blank\">https://wandb.ai/reginaib/kiba_cb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/reginaib/kiba_cb/runs/ztgff617' target=\"_blank\">https://wandb.ai/reginaib/kiba_cb/runs/ztgff617</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = WandbLogger(project='kiba_cb', job_type='train')\n",
    "trainer = Trainer(accelerator='cpu', max_epochs=10, logger=logger)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:57:09.953741700Z",
     "start_time": "2024-02-27T11:57:05.768708400Z"
    }
   },
   "id": "325046c1449664b8"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 139 K \n",
      "1 | classifier | Sequential | 16.6 K\n",
      "------------------------------------------\n",
      "156 K     Trainable params\n",
      "0         Non-trainable params\n",
      "156 K     Total params\n",
      "0.624     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "047b1d1dc470432a98d69497134047bc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rena_\\git\\AC-DTI\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "C:\\Users\\rena_\\git\\AC-DTI\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "564d1d52aa36486cbdbc65f73753a2b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a650d2ebc2e44dd491705162dae086b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb0382cf6e884b8fbd8685572fcf6929"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e183278f2ff475db56bfce613da627c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5bfb536cd96b4324a3cfa008ee42734b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1c72b89969445fa804b784b8deb6287"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "110e67da89a340879a8505d46ce19a69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2dcae83650a6412bba306ee56badbabe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1dc778c75f94dc8ace4e13ac7eb2f44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b091108fe284bf4a551e671f2aa6e13"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94fe7a626e8b4cd59be74c62e6721932"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:00:12.641401300Z",
     "start_time": "2024-02-27T11:58:26.754991900Z"
    }
   },
   "id": "b4f9d106dfed155e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d60f942d86c1f344"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
